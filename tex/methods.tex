\section{Theory and Methods}

\subsection{Heat equation}
We will look at the heat equation in one dimension.
\begin{equation}
    \frac{\partial^2 u(x,t)}{\partial x^2} =\frac{\partial u(x,t)}{\partial t},
    t\geq 0, x\in [0,L]
\end{equation}
or
\begin{equation}\label{eq:heat_eq}
    u_{xx} = u_t,
\end{equation}
with initial conditions
\begin{equation}\label{eq:boundary_x}
    u(x,0)= \sin{(\pi x)} \hspace{0.5cm} 0 \leq x \leq L,
\end{equation}
and with $L=1$ being the length of the $x$-region of interest. The 
boundary conditions are
\begin{align}\label{eq:boundary_t}
    u(0,t) &= 0 \hspace{0.5cm} t \ge 0,
    u(L,t) &= 0 \hspace{0.5cm} t \ge 0.
\end{align}

\subsubsection{Closed-form solution}
The analytical solution to this problem is derived in (Tveito and
Winther, 2005, pp. 90-92)\cite{1}, so we will just present a small
summary of the method here. First we assume that $u(x,t)$ is linear and homogeneous and 
that the equation is separable in the form of $u_k(x,t)=X_k(x)T_k(t)$, where
$k$ refers to it being a particular solution. We can then solve the equation 
by separation of variables to find
\begin{equation*}
    u(x,t)_k=e^{-(k\pi/L)^2t}\sin{\left(\frac{k\pi}{L}x\right)},\ k=1,2,3,..
\end{equation*}
This will then give the family ${u_k}$ of particular solutions. We assume then
that $f(x)$ can be written as a linear combination of the eigenfunctions of
$X(x)$, so that
\begin{equation*}
    f(x)=\sum_{k=1}^{N} c_k \sin{\left(\frac{k\pi}{L}x\right)},
\end{equation*}
where $c_k$ is some constant. Then it follows by linearity that
\begin{equation*}
u(x,t)=\sum_{k=1}^N c_ke^{-(k\pi/L)^2t}\sin{\left(\frac{k\pi}{L}x\right)}.
\end{equation*}

Now inserting that $f(x)=\sin(\pi x)$ and $L=1$ it is easy to see that this
gives $c_1=1$ and all other constants $c_k=0$. This gives us the \textbf{closed-form
solution} of
\begin{equation*}
    u(x,t)=e^{-\pi^2 t}\sin{\left(\pi x\right)}.
\end{equation*}

\subsubsection{Numerical approximation}
We will solve this by so called FTCS-scheme (Forward Time Central Space) and
use the Forward-Euler method for moving in time. This results in

\begin{equation*}
u_t\approx \frac{u(x,t+\Delta t)-u(x,t)}{\Delta t}=\frac{u(x_i,t_j+\Delta t)-u(x_i,t_j)}{\Delta t}
\end{equation*}
and
\begin{equation*}
u_{xx}\approx \frac{u(x+\Delta x,t)-2u(x,t)+u(x-\Delta x,t)}{\Delta x^2},
\end{equation*}
or
\begin{equation*}
    u_{xx}\approx \frac{u(x_i+\Delta x,t_j)-2u(x_i,t_j)+u(x_i-\Delta x,t_j)}{\Delta x^2}.
\end{equation*}
And to simplify notation, let
\begin{equation*}
    u(x_i, t_j + \Delta t)\rightarrow u_{i,j+1},
\end{equation*}
and
\begin{equation*}
    u(x_i+\Delta x, t_j) \rightarrow u_{i+1, j}.
\end{equation*}

Then we re-write the equation as

$$
    \frac{u_{i,j+1}-u_{i,j}}{\Delta t} = \frac{u_{i+1,j}+u_{i-1,j}-2u_{i,j}}{\Delta
    x^2}.
$$
Now we can define $\alpha=\Delta t / \Delta x^2$ to get

\begin{equation}
    u_{i,j+1}=\alpha u_{i-1,j} + (1-2\alpha) u_{i,j}+\alpha u_{i+1,j},
\end{equation}

Where we have discretized $x$ and $t$ so that
$$
    x_i = i\Delta x,\ i=0,1,2,...,n,
$$
and
$$
    t_j = j\Delta t,\ j=0,1,2,...,m.
$$

And this scheme is only numerically stable when the \textbf{Von Neumann
stability criterion} is met:
$$
    \alpha\leq 1/2,
$$ 
as described in (Tveito and
Winther, 2005, pp. 132-133)\cite{1}. And it has a \textbf{truncation
error} of
$O(\Delta x^2)$ (Tveito and Winther, 2005, p. 64)\cite{1}.

We note that this problem can be reduced to solving a matrix system, where
$$
    A=
    \begin{bmatrix}
        1-2\alpha & \alpha & 0 & 0 & ... & 0 \\
        \alpha    & 1-2\alpha & \alpha & 0 ... & 0 \\
        0 & \alpha & 1-2\alpha & \alpha & 0 & ... \\
        ... & ... & ... & ... & ... & ... \\
        0 & ... & ... & 0 & \alpha & 1-2\alpha
    \end{bmatrix}
$$
is a tri-diagonal Topelitz matrix. And we can define a column vector for the
time step as
$$
    V_j = 
    \begin{bmatrix}
        u_{1,j}\\
        u_{2,j}\\
        ...\\
        u_{n,j}
    \end{bmatrix}
$$
so that
\begin{equation}
    V_{j+1} = AV_j.
\end{equation}

And this can be Forward solved using the Toeplitz forward solver algorithm \ref{algo:toeplitz}.

\begin{algorithm}
    \caption{Toeplitz Forward solver algorithm}\label{algo:toeplitz}
    \begin{algorithmic}
        \Require{Spacial step-size $\Delta x$}
        
        \Require{Timespan $T$ and lengthspan $L$.}
        
        \Require{Stability criterion parameter $\alpha$.}
        
        \Require{Function f(x)=u(x,0).}
        
        Calculate diagonal elements: $a=\alpha$, $b=1-2\alpha$.
    
        Calculate first timestep $u(x,0)=f(x)$ and set boundary conditions.
    
        \While{Time is less than $T$}
            
            $u(x, t+\Delta t) = a \cdot u(x-\Delta x, t) + b\cdot u(x,
            t) + a \cdot u(x+\Delta x, t)$.
    
            Set boundary conditions.
        \EndWhile
    
    \end{algorithmic}
    \end{algorithm}
    
    This is a
    method of solving a tri-diagonal matrix system without having to do the matrix
    multiplication. 
    
    For our program we have chosen to hold the entire solution in space and time in
    memory. But it is also possible to iterate in time without holding in memory
    and just save the time-steps you want to look at. But for the 1D case this is
    not a concern on a modern computer as we will look at arrays with a maximum of
    $10^4\times 10^4$, which will be under one Megabyte with 32-bit numbers.
   
\subsection{Solving with Neural Networks}
We will use the \textbf{keras API for TensorFlow} to approximate the heat
equation with a NN. To train the network we will use the analytical solution
with the mean squared error as the cost function. This is done by creating a
matrix $X$ that holds all $x$ and $t$ values and a matrix $u$ with the
corresponding analytical values. We will let $x$ be of length $n=100$ and $t$
be of length $m=100$ for the training of the network. Then for the testing we
will double the size of $n$ and $m$. We do not need to shuffle the dataset or
do a train/test split when testing in this way, although some of the training
set will be included in the test set when doing it this way. This will not matter
as we increase the size of the dataset for the testing and will therefore be
able to spot overfitting if it should happen.

We will set up the network
with 3 hidden layers, with 16 neurons in the first layer, then 32 neurons in
the following two layers. This will be tested using the Adam and RMSprop
optimizers, and ReLu and Sigmoid activation functions. Then we will take the
best performing of these and increase the complexity of the network by adding
new layers of 32 neurons. These will all be trained using a batch size of 30
and 20 epochs.

Then we will look at the time the trained network uses for giving out a
prediction, or feed forwarding, for a two layer network versus the explicit solver. 
Where this network has been trained using RMSprop and has ReLu activation functions.






\begin{comment}
But since the exact solution will
most likely not be reached we can instead try to minimize the given conditions
of the equation. We re-write the heat equation and the boundaries as a minimization problem
\begin{equation*}
    \min_{x,t} \left\{
    \lVert \hat u_{xx} - \hat u_t \rVert^2 
    + \lVert \hat u(0,t) \rVert^2 
    + \lVert \hat u(1,t) \rVert^2
    + \lVert \hat u(x,0)-\sin{(\pi x)}\rVert^2 \right\},
\end{equation*}
where this norm $\lVert \cdot \rVert$ is to be read as the grand sum of the
matrix. We then define our cost/loss-function as
\begin{equation*}
    C(\hat u) =\frac{1}{2} \left[ \lVert \hat u_{xx} - \hat u_t \rVert^2 
    + \lVert \hat u(0,t) \rVert^2 
    + \lVert \hat u(1,t) \rVert^2
    + \lVert \hat u(x,0)-\sin{(\pi x)}\rVert^2 \right]
\end{equation*}

For the backpropagation of our network we need the derivative of this with
respect to the output of the network, which is $\hat u$.
\begin{equation*}
    \frac{\partial C}{\partial\hat u} = 
\end{equation*}
\end{comment}
